---
title: "Unsupervised Class Incremental Learning using Empty Classes"
collection: publications
permalink: /publication/shoeb_bmvc25
excerpt: 'For real-world applications, deep neural networks (DNNs) must recognize and adapt to previously unseen inputs and changing environments. To achieve this, we propose a novel method to augment DNNs with the capability to identify and incrementally learn novel classes that were not present in their initial training set. Our approach uses anomaly detection to retrieve out-of-distribution (OoD) samples as potential candidates for new classes and uses k empty classes to learn these novel classes incrementally in an unsupervised fashion. We introduce two loss functions, which 1) encourage the DNN to allocate OoD samples to the new empty classes and 2) minimize the inner-class feature distance between the newly formed classes. Unlike previous approaches that rely on labeled data for each class, our model uses a single label for all OoD data and a precomputed distance matrix to differentiate between them. Our experiments across image classification and semantic segmentation tasks show our method’s ability to expand a DNN’s understanding by several classes without requiring explicit ground truth labels.'
date: 2024-11-28
venue: 'The 35th British Machine Vision Conference (BMVC)'
slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
paperurl: 'https://bmva-archive.org.uk/bmvc/2024/workshops/RROW/01_Unsupervised_Class_Increment.pdf'
citation: 'Uhlemeyer, S., Lienen, J., Shoeb, Y., Hüllermeier, E. and Gottschalk, H. (2024). &quot; Unsupervised Class Incremental Learning using Empty Classes &quot; <i> Proceedings of British Machine Vision Conference Workshops</i>.'
---

For real-world applications, deep neural networks (DNNs) must recognize and adapt to previously unseen inputs and changing environments. To achieve this, we propose a novel method to augment DNNs with the capability to identify and incrementally learn novel classes that were not present in their initial training set. Our approach uses anomaly detection to retrieve out-of-distribution (OoD) samples as potential candidates for new classes and uses k empty classes to learn these novel classes incrementally in an unsupervised fashion. We introduce two loss functions, which 1) encourage the DNN to allocate OoD samples to the new empty classes and 2) minimize the inner-class feature distance between the newly formed classes. Unlike previous approaches that rely on labeled data for each class, our model uses a single label for all OoD data and a precomputed distance matrix to differentiate between them. Our experiments across image classification and semantic segmentation tasks show our method’s ability to expand a DNN’s understanding by several classes without requiring explicit ground truth labels.